{"cells":[{"cell_type":"code","execution_count":49,"metadata":{"id":"E-mNhUjQuxNM","executionInfo":{"status":"ok","timestamp":1740532683735,"user_tz":480,"elapsed":4,"user":{"displayName":"Luca Labardini","userId":"02118104616351803622"}}},"outputs":[],"source":["import torch\n","import triton\n","import triton.language as tl\n","import time"]},{"cell_type":"code","execution_count":50,"metadata":{"id":"N9lmLw8cuxNN","executionInfo":{"status":"ok","timestamp":1740532684577,"user_tz":480,"elapsed":9,"user":{"displayName":"Luca Labardini","userId":"02118104616351803622"}}},"outputs":[],"source":["def is_cuda():\n","    return triton.runtime.driver.active.get_current_target().backend == \"cuda\""]},{"cell_type":"code","execution_count":51,"metadata":{"id":"eUMlpjFJuxNO","executionInfo":{"status":"ok","timestamp":1740532685225,"user_tz":480,"elapsed":3,"user":{"displayName":"Luca Labardini","userId":"02118104616351803622"}}},"outputs":[],"source":["def is_hip_mi200():\n","    target = triton.runtime.driver.active.get_current_target()\n","    return target.backend == 'hip' and target.arch == 'gfx90a'"]},{"cell_type":"code","execution_count":52,"metadata":{"id":"lBNGYaejuxNO","executionInfo":{"status":"ok","timestamp":1740532687407,"user_tz":480,"elapsed":19,"user":{"displayName":"Luca Labardini","userId":"02118104616351803622"}}},"outputs":[],"source":["\"\"\"\n","PA2 Part 2: MatMul+Relu+Add Fused Optimization.\n","The kernel uses several optimization techniques:\n","\n","  1. Shared memory tiling.\n","  2. Register tiling.\n","  3. Cooperative fetching.\n","  4. Operator Fusion\n","  5. Write cache / epilogue fusion.\n","\n","Fill in the missing parts (marked with TODO).\n","\"\"\"\n","\n","# -----------------------------------------------------------------------------\n","# Tiling parameters - You will need to change these to achieve better results.\n","# -----------------------------------------------------------------------------\n","BLOCK_M = 128#Original: 128  # Tile size in the M dimension.\n","BLOCK_N = 256#Original: 128 # Tile size in the N dimension.\n","BLOCK_K = 16#Original: 32 # Tile size in the K dimension.\n","\n","\n","# -----------------------------------------------------------------------------\n","# Triton Kernel: Matrix Multiplication + ReLU + Add\n","#\n","# The kernel uses:\n","#   Step 1: Tile assignment (each kernel computes a tile of C)\n","#   Step 2: Shared memory tiling + Cooperative Fetching: Load tiles of A and B.\n","#   Step 3: Register tiling: Use a register accumulator.\n","#   Step 4: Add and ReLU fusion\n","#   Step 5: Write cache/Epilogue: Write the final tile back to global memory.\n","# -----------------------------------------------------------------------------\n","@triton.jit\n","def matmul_add_relu_kernel_fp16(\n","    a_ptr, b_ptr, c_ptr, d_ptr,\n","    M: tl.constexpr, N: tl.constexpr, K: tl.constexpr,\n","    stride_am: tl.constexpr, stride_ak: tl.constexpr,\n","    stride_bk: tl.constexpr, stride_bn: tl.constexpr,\n","    stride_cm: tl.constexpr, stride_cn: tl.constexpr,\n","    stride_dm: tl.constexpr, stride_dn: tl.constexpr,\n","    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n","):\n","    # -------------------------------------------------------------------------\n","    # Step 1: Tile: Assignment\n","    #\n","    # Each kernel instance is mapped to a tile in the output matrix C.\n","    # Compute the starting indices (m_start, n_start) for this tile.\n","    # -------------------------------------------------------------------------\n","    # TODO: Compute the tile indices using program_id(0) for M and program_id(1) for N.\n","    pid_m = tl.program_id(0)\n","    pid_n = tl.program_id(1)\n","\n","    m_start = pid_m * BLOCK_M\n","    n_start = pid_n * BLOCK_N\n","\n","    # -------------------------------------------------------------------------\n","    # Step 2: Register Tiling\n","    # -------------------------------------------------------------------------\n","    # TODO: Initialize the accumulator \"acc\" with zeros (dtype: float16).\n","    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float16)\n","\n","    # -------------------------------------------------------------------------\n","    # Step 3: Shared Memory Tiling & Cooperative Fetching.\n","    # Compute pointers to the sub-tiles of A and B that are needed to compute\n","    # the current C tile. The offsets here serve to load BLOCK_SIZE_M x BLOCK_SIZE_K\n","    # and BLOCK_SIZE_K x BLOCK_SIZE_N blocks from A and B respectively.\n","    # -------------------------------------------------------------------------\n","    # TODO: Finish code below\n","    for k in range(0, K, BLOCK_K):\n","\n","        a_indices_m = m_start + tl.arange(0, BLOCK_M)\n","        a_indices_k = k + tl.arange(0, BLOCK_K)\n","        a_ptrs = a_ptr + a_indices_m[:, None] * stride_am + a_indices_k[None, :] * stride_ak\n","        mask_a = (a_indices_m[:, None] < M) & (a_indices_k[None, :] < K)\n","\n","        b_indices_k = k + tl.arange(0, BLOCK_K)\n","        b_indices_n = n_start + tl.arange(0, BLOCK_N)\n","        b_ptrs = b_ptr + b_indices_k[:, None] * stride_bk + b_indices_n[None, :] * stride_bn\n","        mask_b = (b_indices_k[:, None] < K) & (b_indices_n[None, :] < N)\n","\n","        a_tile = tl.cast(tl.load(a_ptrs, mask=mask_a, other=0.0), tl.float16)\n","        b_tile = tl.cast(tl.load(b_ptrs, mask=mask_b, other=0.0), tl.float16)\n","\n","        acc += tl.dot(a_tile, b_tile, out_dtype=tl.float16)\n","    # -------------------------------------------------------------------------\n","    # Step 4: Apply ReLU and Add C to the accumulator\n","    # -------------------------------------------------------------------------\n","    # TODO: Finish code below\n","    row_indices = m_start + tl.arange(0, BLOCK_M)\n","    col_indices = n_start + tl.arange(0, BLOCK_N)\n","\n","    c_ptrs = c_ptr + row_indices[:, None] * stride_cm + col_indices[None, :] * stride_cn\n","    mask_c = (row_indices[:, None] < M) & (col_indices[None, :] < N)\n","    c_tile = tl.load(c_ptrs, mask=mask_c, other=0.0)\n","\n","    acc = acc + c_tile\n","\n","    acc = tl.maximum(acc, 0.0)\n","\n","    # -------------------------------------------------------------------------\n","    # Step 5: Write Cache / Epilogue Fusion: Write the computed tile to D.\n","    # -------------------------------------------------------------------------\n","    # TODO: Finish code below\n","\n","    d_ptrs = d_ptr + row_indices[:, None] * stride_dm + col_indices[None, :] * stride_dn\n","    tl.store(d_ptrs, acc, mask=mask_c)\n","\n",""]},{"cell_type":"code","execution_count":53,"metadata":{"id":"u16sz-IUuxNP","executionInfo":{"status":"ok","timestamp":1740532690596,"user_tz":480,"elapsed":3,"user":{"displayName":"Luca Labardini","userId":"02118104616351803622"}}},"outputs":[],"source":["def matmul_add_relu_fp16(a: torch.Tensor, b: torch.Tensor, c: torch.Tensor) -> torch.Tensor:\n","    \"\"\"\n","    Computes Output = ReLU(A @ B + C) using fp16 precision for maximum throughput.\n","    \"\"\"\n","    M, K = a.shape\n","    K2, N = b.shape\n","    assert K == K2, \"Incompatible dimensions\"\n","\n","    d = torch.empty((M, N), device=a.device, dtype=torch.float16)\n","    # Create launch grid\n","    grid = (triton.cdiv(M, BLOCK_M), triton.cdiv(N, BLOCK_N))\n","\n","    matmul_add_relu_kernel_fp16[grid](\n","        a, b, c, d,\n","        M, N, K,\n","        a.stride(0), a.stride(1),\n","        b.stride(0), b.stride(1),\n","        c.stride(0), c.stride(1),\n","        d.stride(0), d.stride(1),\n","        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K\n","    )\n","    return d"]},{"cell_type":"code","execution_count":54,"metadata":{"id":"AJ7LlTPawPqB","executionInfo":{"status":"ok","timestamp":1740532691835,"user_tz":480,"elapsed":3,"user":{"displayName":"Luca Labardini","userId":"02118104616351803622"}}},"outputs":[],"source":["# Reference implementation using PyTorch\n","def reference_matmul_add_relu(A, B, C):\n","    result = torch.matmul(A, B).add(C).relu_()\n","    return result"]},{"cell_type":"code","execution_count":55,"metadata":{"id":"B4J5ZBpOuxNP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740532694753,"user_tz":480,"elapsed":1441,"user":{"displayName":"Luca Labardini","userId":"02118104616351803622"}},"outputId":"4e91006d-1181-4c0f-afc7-f1a7860c8fc9"},"outputs":[{"output_type":"stream","name":"stdout","text":["triton_output_with_fp16_inputs=tensor([[ 0.0000,  6.1250,  0.0000,  ..., 10.0625,  0.0000,  0.0000],\n","        [ 7.9102, 15.6328, 26.6094,  ..., 11.4609,  5.3750, 18.6250],\n","        [ 2.7246,  0.0000,  0.0000,  ...,  0.0000, 26.0781,  0.0000],\n","        ...,\n","        [ 0.4448, 75.1875,  0.0000,  ..., 26.2812,  0.0000,  0.0000],\n","        [ 6.9492,  1.1230,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [27.6094, 26.9531, 22.9219,  ..., 13.5391,  6.0508, 21.6250]],\n","       device='cuda:0', dtype=torch.float16)\n","torch_output_with_fp16_inputs=tensor([[ 0.0000,  6.1289,  0.0000,  ..., 10.0391,  0.0000,  0.0000],\n","        [ 7.9102, 15.6328, 26.6250,  ..., 11.4531,  5.3945, 18.6562],\n","        [ 2.7266,  0.0000,  0.0000,  ...,  0.0000, 26.1250,  0.0000],\n","        ...,\n","        [ 0.4316, 75.2500,  0.0000,  ..., 26.2812,  0.0000,  0.0000],\n","        [ 6.9570,  1.1260,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n","        [27.6406, 26.9531, 22.9375,  ..., 13.5625,  6.0391, 21.6406]],\n","       device='cuda:0', dtype=torch.float16)\n","✅ Triton and Torch match\n"]}],"source":["# -----------------------------------------------------------------------------\n","# Accuracy Tests\n","# -----------------------------------------------------------------------------\n","if __name__ == \"__main__\":\n","    torch.manual_seed(0)\n","    a = torch.randn((512, 512), device=torch.device(\"cuda\"), dtype=torch.float16)\n","    b = torch.randn((512, 512), device=torch.device(\"cuda\"), dtype=torch.float16)\n","    c = torch.randn((512, 512), device=torch.device(\"cuda\"), dtype=torch.float16)\n","    triton_output = matmul_add_relu_fp16(a, b, c)\n","    torch_output = reference_matmul_add_relu(a, b, c)\n","    print(f\"triton_output_with_fp16_inputs={triton_output}\")\n","    print(f\"torch_output_with_fp16_inputs={torch_output}\")\n","    rtol = 1e-2 if is_hip_mi200() else 0.032\n","    if torch.allclose(triton_output, torch_output, atol=0.15, rtol=rtol):\n","        print(\"✅ Triton and Torch match\")\n","    else:\n","        diff = triton_output - torch_output\n","        abs_diff = torch.abs(diff)\n","        max_abs_diff = torch.max(abs_diff)\n","        print(f\"❌ Triton and Torch differ: {max_abs_diff=}\")"]},{"cell_type":"code","execution_count":56,"metadata":{"id":"kj_dGOlazQJY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740532709582,"user_tz":480,"elapsed":11142,"user":{"displayName":"Luca Labardini","userId":"02118104616351803622"}},"outputId":"b4a21835-4d71-42d0-817b-1e399996194b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Triton implementation\n","PyTorch implementation\n","Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n","Triton implementation: 0.77 ms\n","PyTorch implementation: 1.07 ms\n","\n","Speedup of Triton vs PyTorch: 1.40x\n"]}],"source":["# -----------------------------------------------------------------------------\n","# Performance Benchmark\n","# IMPORTANT: DO NOT CHANGE THIS CODE.\n","# THIS IS THE EXACT CODE THAT WILL BE USED TO GRADE YOUR IMPLEMENTATION.\n","# ANY CHANGES TO THIS CODE (INCLUDING DIMENSIONS, REPEATS, etc.)\n","# WILL CAUSE YOU TO HAVE DIFFERENT SPEEDUP RESULTS.\n","# -----------------------------------------------------------------------------\n","M = 2048\n","K = 2048\n","N = 2048\n","\n","# KEEP THESE MATRICES IN FP16. FP32 WILL NOT PROVIDE ACCURATE RESULTS\n","A = torch.randn((M, K), device=\"cuda\", dtype=torch.float16)\n","B = torch.randn((K, N), device=\"cuda\", dtype=torch.float16)\n","C = torch.randn((M, N), device=\"cuda\", dtype=torch.float16)\n","\n","# warmup\n","_ = matmul_add_relu_fp16(A, B, C)\n","_ = reference_matmul_add_relu(A, B, C)\n","\n","REPEATS = 5000\n","\n","# time your implementation\n","print(\"Triton implementation\")\n","torch.cuda.synchronize()\n","start = time.perf_counter()\n","for _ in range(REPEATS):\n","    _ = matmul_add_relu_fp16(A, B, C)\n","torch.cuda.synchronize()\n","triton_time = (time.perf_counter() - start) / REPEATS\n","\n","# time pytorch\n","print(\"PyTorch implementation\")\n","torch.cuda.synchronize()\n","start = time.perf_counter()\n","for _ in range(REPEATS):\n","    _ = reference_matmul_add_relu(A, B, C)\n","torch.cuda.synchronize()\n","torch_time = (time.perf_counter() - start) / REPEATS\n","\n","print(f\"Performance comparison for matrix multiplication ({M}x{K} @ {K}x{N}):\")\n","print(f\"Triton implementation: {triton_time*1000:.2f} ms\")\n","print(f\"PyTorch implementation: {torch_time*1000:.2f} ms\")\n","\n","print(f\"\\nSpeedup of Triton vs PyTorch: {torch_time/triton_time:.2f}x\")"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"K9Hdpxic0tq6","executionInfo":{"status":"ok","timestamp":1740531737945,"user_tz":480,"elapsed":3,"user":{"displayName":"Luca Labardini","userId":"02118104616351803622"}}},"outputs":[],"source":["# Write your grid search here.\n","\n","def matmul_add_relu_fp16_hyper(a: torch.Tensor, b: torch.Tensor, c: torch.Tensor, block_m: int, block_n: int, block_k: int) -> torch.Tensor:\n","    \"\"\"\n","    Computes Output = ReLU(A @ B + C) using fp16 precision for maximum throughput.\n","    \"\"\"\n","    M, K = a.shape\n","    K2, N = b.shape\n","    assert K == K2, \"Incompatible dimensions\"\n","\n","    d = torch.empty((M, N), device=a.device, dtype=torch.float16)\n","    # Create launch grid\n","    grid = (triton.cdiv(M, block_m), triton.cdiv(N, block_n))\n","\n","    matmul_add_relu_kernel_fp16[grid](\n","        a, b, c, d,\n","        M, N, K,\n","        a.stride(0), a.stride(1),\n","        b.stride(0), b.stride(1),\n","        c.stride(0), c.stride(1),\n","        d.stride(0), d.stride(1),\n","        BLOCK_M=block_m, BLOCK_N=block_n, BLOCK_K=block_k\n","    )\n","    return d\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","source":["M = 2048\n","K = 2048\n","N = 2048\n","\n","# KEEP THESE MATRICES IN FP16. FP32 WILL NOT PROVIDE ACCURATE RESULTS\n","A = torch.randn((M, K), device=\"cuda\", dtype=torch.float16)\n","B = torch.randn((K, N), device=\"cuda\", dtype=torch.float16)\n","C = torch.randn((M, N), device=\"cuda\", dtype=torch.float16)\n","\n","# warmup\n","_ = matmul_add_relu_fp16(A, B, C)\n","_ = reference_matmul_add_relu(A, B, C)\n","\n","REPEATS = 5000\n","\n","import time\n","\n","# Some candidate sets for the hyperparameters.\n","candidate_block_m = [128, 256]\n","candidate_block_n = [128, 256]\n","candidate_block_k = [16, 32]\n","\n","\n","for bn in candidate_block_n:\n","    for bm in candidate_block_m:\n","        for bk in candidate_block_k:\n","\n","            print(f\"Candidate Block Set BLOCK_N = {bn}, BLOCK_M = {bm}, BLOCK_K= {bk}\\n\")\n","\n","\n","            # warmup\n","            _ = matmul_add_relu_fp16(A, B, C)\n","            _ = reference_matmul_add_relu(A, B, C)\n","\n","            REPEATS = 5000\n","\n","            # time your implementation\n","            print(\"Triton implementation\")\n","            torch.cuda.synchronize()\n","            start = time.perf_counter()\n","            for _ in range(REPEATS):\n","                _ = matmul_add_relu_fp16_hyper(A, B, C, block_m=bm, block_k=bk, block_n=bn)\n","            torch.cuda.synchronize()\n","            triton_time = (time.perf_counter() - start) / REPEATS\n","\n","            # time pytorch\n","            print(\"PyTorch implementation\")\n","            torch.cuda.synchronize()\n","            start = time.perf_counter()\n","            for _ in range(REPEATS):\n","                _ = reference_matmul_add_relu(A, B, C)\n","            torch.cuda.synchronize()\n","            torch_time = (time.perf_counter() - start) / REPEATS\n","\n","            print(f\"Performance comparison for matrix multiplication ({M}x{K} @ {K}x{N}):\")\n","            print(f\"Triton implementation: {triton_time*1000:.2f} ms\")\n","            print(f\"PyTorch implementation: {torch_time*1000:.2f} ms\")\n","\n","            print(f\"\\nSpeedup of Triton vs PyTorch: {torch_time/triton_time:.2f}x\")\n","            print(\"\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WdkrbzhkqOD8","executionInfo":{"status":"ok","timestamp":1740532438110,"user_tz":480,"elapsed":99782,"user":{"displayName":"Luca Labardini","userId":"02118104616351803622"}},"outputId":"4b0a9470-caca-4996-8c56-46e37af3c384"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["Candidate Block Set BLOCK_N = 128, BLOCK_M = 128, BLOCK_K= 16\n","\n","Triton implementation\n","PyTorch implementation\n","Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n","Triton implementation: 0.91 ms\n","PyTorch implementation: 1.06 ms\n","\n","Speedup of Triton vs PyTorch: 1.16x\n","\n","\n","Candidate Block Set BLOCK_N = 128, BLOCK_M = 128, BLOCK_K= 32\n","\n","Triton implementation\n","PyTorch implementation\n","Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n","Triton implementation: 1.06 ms\n","PyTorch implementation: 1.10 ms\n","\n","Speedup of Triton vs PyTorch: 1.04x\n","\n","\n","Candidate Block Set BLOCK_N = 128, BLOCK_M = 256, BLOCK_K= 16\n","\n","Triton implementation\n","PyTorch implementation\n","Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n","Triton implementation: 0.85 ms\n","PyTorch implementation: 1.14 ms\n","\n","Speedup of Triton vs PyTorch: 1.33x\n","\n","\n","Candidate Block Set BLOCK_N = 128, BLOCK_M = 256, BLOCK_K= 32\n","\n","Triton implementation\n","PyTorch implementation\n","Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n","Triton implementation: 0.87 ms\n","PyTorch implementation: 1.11 ms\n","\n","Speedup of Triton vs PyTorch: 1.28x\n","\n","\n","Candidate Block Set BLOCK_N = 256, BLOCK_M = 128, BLOCK_K= 16\n","\n","Triton implementation\n","PyTorch implementation\n","Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n","Triton implementation: 0.79 ms\n","PyTorch implementation: 1.08 ms\n","\n","Speedup of Triton vs PyTorch: 1.37x\n","\n","\n","Candidate Block Set BLOCK_N = 256, BLOCK_M = 128, BLOCK_K= 32\n","\n","Triton implementation\n","PyTorch implementation\n","Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n","Triton implementation: 0.80 ms\n","PyTorch implementation: 1.05 ms\n","\n","Speedup of Triton vs PyTorch: 1.31x\n","\n","\n","Candidate Block Set BLOCK_N = 256, BLOCK_M = 256, BLOCK_K= 16\n","\n","Triton implementation\n","PyTorch implementation\n","Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n","Triton implementation: 1.90 ms\n","PyTorch implementation: 1.04 ms\n","\n","Speedup of Triton vs PyTorch: 0.55x\n","\n","\n","Candidate Block Set BLOCK_N = 256, BLOCK_M = 256, BLOCK_K= 32\n","\n","Triton implementation\n","PyTorch implementation\n","Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n","Triton implementation: 4.12 ms\n","PyTorch implementation: 1.05 ms\n","\n","Speedup of Triton vs PyTorch: 0.26x\n","\n","\n"]}]}],"metadata":{"colab":{"provenance":[],"toc_visible":true,"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}